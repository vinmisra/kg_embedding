{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_kg_in = '/Users/vmisra/data/kg_embed_data/data/traindata_db233_minmentions10_minentity3.pkl'\n",
    "path_wordnet_in = '/Users/vmisra/data/kg_embed_data/data/noun_relations.pkl'\n",
    "\n",
    "SEED = 2052016\n",
    "VALIDATION_HOLDOUT = 0.001\n",
    "N_TRAINING_SAMPLES = 10000\n",
    "N_VAL_SAMPLES = 1400\n",
    "DIST_TYPE = 'UNIGRAM' #'UNIGRAM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Construct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data,_ = pickle.load(open(path_kg_in,'r'))\n",
    "\n",
    "#first, remove all diagonal self-links, as this leads to leakage in the prediction task\n",
    "data = data[np.nonzero(data[:,0]-data[:,1])]\n",
    "\n",
    "#next, shuffle it\n",
    "randstate = np.random.RandomState(SEED)\n",
    "data = randstate.permutation(data)\n",
    "\n",
    "#finally, determine the number of entities for future reference\n",
    "n_entities = max(max(data[:,0]),max(data[:,1]))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next, remove the validation set\n",
    "data_trainable, data_validate = train_test_split(data,test_size = VALIDATION_HOLDOUT, random_state=SEED)\n",
    "data_train_pos = data_trainable[:N_TRAINING_SAMPLES].copy()\n",
    "data_val_pos = data_validate[:N_VAL_SAMPLES].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate negative samples for training\n",
    "if DIST_TYPE == 'UNIGRAM':\n",
    "    randidxs = randstate.permutation(data.flatten())[:N_TRAINING_SAMPLES] #unigram distribution\n",
    "elif DIST_TYPE == 'UNIFORM':\n",
    "    randidxs = randstate.randint(n_entities,size=N_TRAINING_SAMPLES)#uniform distribution\n",
    "else:\n",
    "    print 'UNRECOGNIZED DISTRIBUTION TYPE!!!'\n",
    "    \n",
    "randchoice = randstate.binomial(n=1,p=.5,size=(N_TRAINING_SAMPLES))\n",
    "data_train_neg = data_train_pos.copy()\n",
    "\n",
    "for edge,choice,randidx in zip(data_train_neg,randchoice,randidxs):\n",
    "    edge[choice] = randidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate negative samples for validation\n",
    "if DIST_TYPE == 'UNIGRAM':\n",
    "    randidxs = randstate.permutation(data.flatten())[:N_TRAINING_SAMPLES] #unigram distribution\n",
    "elif DIST_TYPE == 'UNIFORM':\n",
    "    randidxs = randstate.randint(n_entities,size=N_TRAINING_SAMPLES)#uniform distribution\n",
    "else:\n",
    "    print 'UNRECOGNIZED DISTRIBUTION TYPE!!!'\n",
    "randchoice = randstate.binomial(n=1,p=.5,size=(N_VAL_SAMPLES))\n",
    "data_val_neg = data_val_pos.copy()\n",
    "\n",
    "for edge,choice,randidx in zip(data_val_neg,randchoice,randidxs):\n",
    "    edge[choice] = randidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = np.transpose(data)[0]\n",
    "cols = np.transpose(data)[1]\n",
    "\n",
    "graph = csr_matrix((np.ones(len(rows)+len(cols)),(np.concatenate([rows,cols]),np.concatenate([cols,rows]))),shape=(n_entities,n_entities))\n",
    "graph.data = np.ones(len(graph.data))\n",
    "#graph.setdiag(values=np.zeros(graph.shape[0])) #no longer necessary --- we found a simpler way of doing this without the slow scipy sparse operations.\n",
    "#for i in range(graph.shape[0]):\n",
    "#    graph[i,i] = 0 #remove all self-links, as this ensures no leakage in the training process.\n",
    "#graph = csr_matrix((np.ones(len(rows)),(rows,cols)),shape=(n_entities,n_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature computation mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##First: common neighbors vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#common neighbors benchmark\n",
    "def get_common_neighbors(graph_local, data_local):\n",
    "    common_neighbors = np.ndarray(len(data_local))\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for i,edge in enumerate(data_local):\n",
    "        common_neighbors[i] = graph_local.getrow(edge[0]).dot(graph_local.getrow(edge[1]).transpose()).toarray()[0,0]\n",
    "        \n",
    "    print time.time()-start, \" seconds taken for getting common neighbors\"\n",
    "\n",
    "    return common_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Second: Adamic adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_AA(graph_local, data_local, func=lambda x: 1/(1.0+math.log(x+1))):\n",
    "    #neighborhood sizes for each node\n",
    "    n_neighbors = np.squeeze(np.array(graph_local.sum(axis=1)))\n",
    "    #map it into the AA weights as given by the function argument\n",
    "    AA_weights = map(func,n_neighbors)\n",
    "    \n",
    "    #actually compute AA features\n",
    "    AA_features = np.ndarray(len(data_local))\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for i,edge in enumerate(data_local):\n",
    "        AA_features[i] = np.squeeze(np.array(graph_local.getrow(edge[0]).dot(graph_local.getrow(edge[1]).multiply(AA_weights).transpose())))    \n",
    "    print time.time()-start, \"seconds to do AA.\"\n",
    "    \n",
    "    return AA_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Featurize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.81413292885  seconds taken for getting common neighbors\n",
      "8.79685783386  seconds taken for getting common neighbors\n",
      "1.18985891342  seconds taken for getting common neighbors\n",
      "1.21832609177  seconds taken for getting common neighbors\n"
     ]
    }
   ],
   "source": [
    "common_neighbors_pos = get_common_neighbors(graph,data_train_pos)\n",
    "common_neighbors_neg = get_common_neighbors(graph,data_train_neg)\n",
    "common_neighbors_pos_val = get_common_neighbors(graph,data_val_pos)\n",
    "common_neighbors_neg_val = get_common_neighbors(graph,data_val_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.7818379402 seconds to do AA.\n",
      "46.0954511166 seconds to do AA.\n",
      "113.407979012 seconds to do AA.\n",
      "112.447139978 seconds to do AA.\n",
      "113.492281914 seconds to do AA.\n",
      "112.45082593 seconds to do AA.\n",
      "6.01260399818 seconds to do AA.\n",
      "6.06384801865 seconds to do AA.\n",
      "15.3239870071 seconds to do AA.\n",
      "15.1517410278 seconds to do AA.\n",
      "14.9983279705 seconds to do AA.\n",
      "14.9601199627 seconds to do AA.\n"
     ]
    }
   ],
   "source": [
    "straight_AA_pos = gen_AA(graph, data_train_pos)\n",
    "straight_AA_neg = gen_AA(graph, data_train_neg)\n",
    "sqrt_AA_pos = gen_AA(graph,data_train_pos,lambda x: (1+x)**(-.5))\n",
    "sqrt_AA_neg = gen_AA(graph,data_train_neg,lambda x: (1+x)**(-.5))\n",
    "cubert_AA_pos = gen_AA(graph,data_train_pos,lambda x: (1+x)**(-.3))\n",
    "cubert_AA_neg = gen_AA(graph,data_train_neg,lambda x: (1+x)**(-.3))\n",
    "\n",
    "straight_AA_pos_val = gen_AA(graph, data_val_pos)\n",
    "straight_AA_neg_val = gen_AA(graph, data_val_neg)\n",
    "sqrt_AA_pos_val = gen_AA(graph,data_val_pos,lambda x: (1+x)**(-.5))\n",
    "sqrt_AA_neg_val = gen_AA(graph,data_val_neg,lambda x: (1+x)**(-.5))\n",
    "cubert_AA_pos_val = gen_AA(graph,data_val_pos,lambda x: (1+x)**(-.3))\n",
    "cubert_AA_neg_val = gen_AA(graph,data_val_neg,lambda x: (1+x)**(-.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_features_pos = [straight_AA_pos,sqrt_AA_pos,cubert_AA_pos, common_neighbors_pos]\n",
    "raw_features_neg = [straight_AA_neg,sqrt_AA_neg,cubert_AA_neg, common_neighbors_neg]\n",
    "\n",
    "raw_features_pos_val = [straight_AA_pos_val,sqrt_AA_pos_val,cubert_AA_pos_val,common_neighbors_pos_val]\n",
    "raw_features_neg_val = [straight_AA_neg_val,sqrt_AA_neg_val,cubert_AA_neg_val,common_neighbors_neg_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10000, 10000, 10000, 10000]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len,raw_features_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformations = [lambda x: x,\n",
    "                   lambda x: np.log(x+1),\n",
    "                   lambda x: x**.5,\n",
    "                   lambda x: x**.3,\n",
    "                   lambda x: x**2]\n",
    "def transform_features(feature_list):\n",
    "    output_features = []\n",
    "    for transform in transformations:\n",
    "        for feature in feature_list:\n",
    "            output_features.append(transform(feature))\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_pos = np.vstack(transform_features(raw_features_pos)).transpose()\n",
    "features_neg = np.vstack(transform_features(raw_features_neg)).transpose()\n",
    "features_train = np.concatenate([features_pos,features_neg],axis=0)\n",
    "labels_train = np.concatenate([np.ones(len(features_pos)),0*np.ones(len(features_pos))]).astype(np.int)\n",
    "\n",
    "features_pos_val = np.vstack(transform_features(raw_features_pos_val)).transpose()\n",
    "features_neg_val = np.vstack(transform_features(raw_features_neg_val)).transpose()\n",
    "features_val = np.concatenate([features_pos_val,features_neg_val],axis=0)\n",
    "labels_val = np.concatenate([np.ones(len(features_pos_val)),0*np.ones(len(features_pos_val))]).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "featuresScaled_train_val = StandardScaler().fit_transform(np.concatenate([features_train,features_val],axis=0))\n",
    "scaled_feats_train = featuresScaled_train_val[:len(features_train)]\n",
    "scaled_feats_val = featuresScaled_train_val[len(features_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1,penalty='l1')\n",
    "clf.fit(scaled_feats_train,labels_train.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.score(scaled_feats_val,labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neglogprobs = -clf.predict_log_proba(scaled_feats_val)\n",
    "score = 0\n",
    "for logprobs,label in zip(neglogprobs,labels_val):\n",
    "    score += logprobs[label]\n",
    "score/len(labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
