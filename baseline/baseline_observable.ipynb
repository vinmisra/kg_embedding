{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_kg_in = '/Users/vmisra/data/kg_embed_data/data/traindata_db233_minmentions10_minentity3.pkl'\n",
    "path_wordnet_in = '/Users/vmisra/data/kg_embed_data/data/noun_relations.pkl'\n",
    "\n",
    "SEED = 2052016\n",
    "VALIDATION_HOLDOUT = 0.05\n",
    "N_TRAINING_SAMPLES = 100000\n",
    "N_VAL_SAMPLES = -1\n",
    "DIST_TYPE = 'UNIFORM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Construct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data,_,_ = pickle.load(open(path_wordnet_in,'r'))\n",
    "\n",
    "#first, remove all diagonal self-links, as this leads to leakage in the prediction task\n",
    "data = data[np.nonzero(data[:,0]-data[:,1])]\n",
    "\n",
    "#next, shuffle it\n",
    "randstate = np.random.RandomState(SEED)\n",
    "data = randstate.permutation(data)\n",
    "\n",
    "#finally, determine the number of entities for future reference\n",
    "n_entities = max(max(data[:,0]),max(data[:,1]))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next, remove the validation set\n",
    "data_trainable, data_validate = train_test_split(data,test_size = VALIDATION_HOLDOUT, random_state=SEED)\n",
    "data_train_pos = data_trainable[:N_TRAINING_SAMPLES].copy()\n",
    "data_val_pos = data_validate[:N_VAL_SAMPLES].copy()\n",
    "N_VAL_SAMPLES = len(data_val_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate negative samples for training\n",
    "if DIST_TYPE == 'UNIGRAM':\n",
    "    randidxs = randstate.permutation(data.flatten())[:N_TRAINING_SAMPLES] #unigram distribution\n",
    "elif DIST_TYPE == 'UNIFORM':\n",
    "    randidxs = randstate.randint(n_entities,size=N_TRAINING_SAMPLES)#uniform distribution\n",
    "else:\n",
    "    print 'UNRECOGNIZED DISTRIBUTION TYPE!!!'\n",
    "    \n",
    "randchoice = randstate.binomial(n=1,p=.5,size=(N_TRAINING_SAMPLES))\n",
    "data_train_neg = data_train_pos.copy()\n",
    "\n",
    "for edge,choice,randidx in zip(data_train_neg,randchoice,randidxs):\n",
    "    edge[choice] = randidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate negative samples for validation\n",
    "if DIST_TYPE == 'UNIGRAM':\n",
    "    randidxs = randstate.permutation(data.flatten())[:N_TRAINING_SAMPLES] #unigram distribution\n",
    "elif DIST_TYPE == 'UNIFORM':\n",
    "    randidxs = randstate.randint(n_entities,size=N_TRAINING_SAMPLES)#uniform distribution\n",
    "else:\n",
    "    print 'UNRECOGNIZED DISTRIBUTION TYPE!!!'\n",
    "randchoice = randstate.binomial(n=1,p=.5,size=(N_VAL_SAMPLES))\n",
    "data_val_neg = data_val_pos.copy()\n",
    "\n",
    "for edge,choice,randidx in zip(data_val_neg,randchoice,randidxs):\n",
    "    edge[choice] = randidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows = np.transpose(data)[0]\n",
    "cols = np.transpose(data)[1]\n",
    "\n",
    "graph = csr_matrix((np.ones(len(rows)+len(cols)),(np.concatenate([rows,cols]),np.concatenate([cols,rows]))),shape=(n_entities,n_entities))\n",
    "graph.data = np.ones(len(graph.data))\n",
    "#graph.setdiag(values=np.zeros(graph.shape[0])) #no longer necessary --- we found a simpler way of doing this without the slow scipy sparse operations.\n",
    "#for i in range(graph.shape[0]):\n",
    "#    graph[i,i] = 0 #remove all self-links, as this ensures no leakage in the training process.\n",
    "#graph = csr_matrix((np.ones(len(rows)),(rows,cols)),shape=(n_entities,n_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature computation mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##First: common neighbors vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#common neighbors benchmark\n",
    "def get_common_neighbors(graph_local, data_local):\n",
    "    common_neighbors = np.ndarray(len(data_local))\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for i,edge in enumerate(data_local):\n",
    "        common_neighbors[i] = graph_local.getrow(edge[0]).dot(graph_local.getrow(edge[1]).transpose()).toarray()[0,0]\n",
    "        \n",
    "    print time.time()-start, \" seconds taken for getting common neighbors\"\n",
    "    sys.stdout.flush()\n",
    "    return common_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Second: Adamic adar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gen_AA(graph_local, data_local, func=lambda x: 1/(1.0+math.log(x+1))):\n",
    "    #neighborhood sizes for each node\n",
    "    n_neighbors = np.squeeze(np.array(graph_local.sum(axis=1)))\n",
    "    #map it into the AA weights as given by the function argument\n",
    "    AA_weights = map(func,n_neighbors)\n",
    "    \n",
    "    #actually compute AA features\n",
    "    AA_features = np.ndarray(len(data_local))\n",
    "    import time\n",
    "    start = time.time()\n",
    "    for i,edge in enumerate(data_local):\n",
    "        AA_features[i] = np.squeeze(np.array(graph_local.getrow(edge[0]).dot(graph_local.getrow(edge[1]).multiply(AA_weights).transpose())))    \n",
    "    print time.time()-start, \"seconds to do AA.\"\n",
    "    sys.stdout.flush()\n",
    "    return AA_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Featurize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.9058570862  seconds taken for getting common neighbors\n",
      "84.8270850182  seconds taken for getting common neighbors\n"
     ]
    }
   ],
   "source": [
    "common_neighbors_pos = get_common_neighbors(graph,data_train_pos)\n",
    "common_neighbors_neg = get_common_neighbors(graph,data_train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.71619200706  seconds taken for getting common neighbors\n",
      "9.55100798607  seconds taken for getting common neighbors\n"
     ]
    }
   ],
   "source": [
    "common_neighbors_pos_val = get_common_neighbors(graph,data_val_pos)\n",
    "common_neighbors_neg_val = get_common_neighbors(graph,data_val_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394.50590992 seconds to do AA.\n",
      "396.158967018 seconds to do AA.\n",
      "922.640341043 seconds to do AA.\n",
      "914.378906012 seconds to do AA.\n",
      "915.507543802 seconds to do AA.\n",
      "917.92914319 seconds to do AA.\n"
     ]
    }
   ],
   "source": [
    "straight_AA_pos = gen_AA(graph, data_train_pos)\n",
    "straight_AA_neg = gen_AA(graph, data_train_neg)\n",
    "sqrt_AA_pos = gen_AA(graph,data_train_pos,lambda x: (1+x)**(-.5))\n",
    "sqrt_AA_neg = gen_AA(graph,data_train_neg,lambda x: (1+x)**(-.5))\n",
    "cubert_AA_pos = gen_AA(graph,data_train_pos,lambda x: (1+x)**(-.3))\n",
    "cubert_AA_neg = gen_AA(graph,data_train_neg,lambda x: (1+x)**(-.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.387305975 seconds to do AA.\n",
      "45.9769368172 seconds to do AA.\n",
      "106.259618998 seconds to do AA.\n",
      "106.118647099 seconds to do AA.\n",
      "105.348823071 seconds to do AA.\n",
      "105.568843126 seconds to do AA.\n"
     ]
    }
   ],
   "source": [
    "straight_AA_pos_val = gen_AA(graph, data_val_pos)\n",
    "straight_AA_neg_val = gen_AA(graph, data_val_neg)\n",
    "sqrt_AA_pos_val = gen_AA(graph,data_val_pos,lambda x: (1+x)**(-.5))\n",
    "sqrt_AA_neg_val = gen_AA(graph,data_val_neg,lambda x: (1+x)**(-.5))\n",
    "cubert_AA_pos_val = gen_AA(graph,data_val_pos,lambda x: (1+x)**(-.3))\n",
    "cubert_AA_neg_val = gen_AA(graph,data_val_neg,lambda x: (1+x)**(-.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_features_pos = [straight_AA_pos,sqrt_AA_pos,cubert_AA_pos, common_neighbors_pos]\n",
    "raw_features_neg = [straight_AA_neg,sqrt_AA_neg,cubert_AA_neg, common_neighbors_neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_features_pos_val = [straight_AA_pos_val,sqrt_AA_pos_val,cubert_AA_pos_val,common_neighbors_pos_val]\n",
    "raw_features_neg_val = [straight_AA_neg_val,sqrt_AA_neg_val,cubert_AA_neg_val,common_neighbors_neg_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100000, 100000, 100000, 100000]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len,raw_features_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformations = [lambda x: x,\n",
    "                   lambda x: np.log(x+1),\n",
    "                   lambda x: x**.5,\n",
    "                   lambda x: x**.3,\n",
    "                   lambda x: x**2]\n",
    "\n",
    "def transform_features(feature_list):\n",
    "    output_features = []\n",
    "    for transform in transformations:\n",
    "        for feature in feature_list:\n",
    "            output_features.append(transform(feature))\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_pos = np.vstack(transform_features(raw_features_pos)).transpose()\n",
    "features_neg = np.vstack(transform_features(raw_features_neg)).transpose()\n",
    "features_train = np.concatenate([features_pos,features_neg],axis=0)\n",
    "labels_train = np.concatenate([np.ones(len(features_pos)),0*np.ones(len(features_pos))]).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_pos_val = np.vstack(transform_features(raw_features_pos_val)).transpose()\n",
    "features_neg_val = np.vstack(transform_features(raw_features_neg_val)).transpose()\n",
    "features_val = np.concatenate([features_pos_val,features_neg_val],axis=0)\n",
    "labels_val = np.concatenate([np.ones(len(features_pos_val)),0*np.ones(len(features_pos_val))]).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "featuresScaled_train_val = StandardScaler().fit_transform(np.concatenate([features_train,features_val],axis=0))\n",
    "scaled_feats_train = featuresScaled_train_val[:len(features_train)]\n",
    "scaled_feats_val = featuresScaled_train_val[len(features_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, penalty='l1', random_state=None, tol=0.0001)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=10,penalty='l1')\n",
    "clf.fit(scaled_feats_train,labels_train.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.66273389e-01,  -2.53564458e-01,   4.85040230e-01,\n",
       "         -4.36690716e-01,  -5.60143261e-01,   7.55117520e-04,\n",
       "          3.91808745e-01,  -2.00667252e-01,  -2.12913207e-01,\n",
       "          1.97231585e-01,   4.28340863e-01,  -5.20653450e-02,\n",
       "          3.42887019e-01,   4.76192743e-01,   2.74870029e-01,\n",
       "          2.56046196e-01,   0.00000000e+00,  -1.00150826e+00,\n",
       "          1.23946834e+00,   1.07403587e+00]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54470842332613389"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(scaled_feats_val,labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 20)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_feats_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.66291875249218735"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neglogprobs = -clf.predict_log_proba(scaled_feats_val)\n",
    "score = 0\n",
    "for logprobs,label in zip(neglogprobs,labels_val):\n",
    "    score += logprobs[label]\n",
    "score/len(labels_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
